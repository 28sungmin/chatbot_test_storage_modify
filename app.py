import os, json, pickle, numpy as np
from datetime import datetime, timedelta
import tiktoken
from dotenv import load_dotenv, find_dotenv
import streamlit as st
import faiss
from openai import OpenAI
import re
from numpy.linalg import norm
import redis
from streamlit_js_eval import streamlit_js_eval
import hashlib
from typing import Tuple, Dict, Any

#===================================================================================
# ê¸°ë³¸ ì„¤ì •
#===================================================================================
#-----------------------
# Redis & OpenAI ì„¤ì •
#-----------------------
load_dotenv(find_dotenv(), override=True)

api_key = os.environ.get('OPENAI_API_KEY')
client = OpenAI(api_key=api_key)

redis_host = os.environ.get("REDIS_HOST")
redis_port = int(os.environ.get("REDIS_PORT"))
redis_password = os.environ.get("REDIS_PASSWORD")

EMBED_MODEL     = "text-embedding-3-small"
CHAT_MODEL_GENERAL      = "gpt-4.1"
CHAT_MODEL_MINI      = "gpt-4o-mini"
TOP_K           = 4
r = redis.Redis(
    host=redis_host,
    port=redis_port,
    decode_responses=True,
    username="default",
    password=redis_password,
)
# Redis ì—°ê²° í™•ì¸
try:
    r.ping()
except Exception as e:
    st.error(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")

#-----------------------------------
# ê° ê°€ì´ë“œë¼ì¸ë³„ .indexì™€ .pkl íŒŒì¼ ì„¤ì •
#-----------------------------------
# 1ê¶Œ
IDX_FILE_1        = "data/book1_faiss_chunk_250804.index"
META_FILE_1       = "data/book1_meta_chunk_250804.pkl"
SECTION_IDX_FILE_1 = "data/book1_faiss_section_keywords_250804.index"
SECTION_META_FILE_1 = "data/book1_meta_section_keywords_250804.pkl"
PAGE_IDX_FILE_1 = "data/book1_faiss_page_250804.index"
PAGE_META_FILE_1 = "data/book1_meta_page_250804.pkl"
# 2ê¶Œ
IDX_FILE_2        = "data/book2_faiss_chunk_250804.index"
META_FILE_2       = "data/book2_meta_chunk_250804.pkl"
SECTION_IDX_FILE_2 = "data/book2_faiss_section_keywords_250804.index"
SECTION_META_FILE_2 = "data/book2_meta_section_keywords_250804.pkl"
PAGE_IDX_FILE_2 = "data/book2_faiss_page_250804.index"
PAGE_META_FILE_2 = "data/book2_meta_page_250804.pkl"
# 3ê¶Œ
IDX_FILE_3        = "data/book3_faiss_chunk_250801.index"
META_FILE_3       = "data/book3_meta_chunk_250801.pkl"
SECTION_IDX_FILE_3 = "data/book3_faiss_section_keywords_250801.index"
SECTION_META_FILE_3 = "data/book3_meta_section_keywords_250801.pkl"
PAGE_IDX_FILE_3 = "data/book3_faiss_page_250801.index"
PAGE_META_FILE_3 = "data/book3_meta_page_250801.pkl"
# 4ê¶Œ
IDX_FILE_4        = "data/book4_faiss_chunk_table_250808.index"
META_FILE_4       = "data/book4_meta_chunk_table_250808.pkl"
SECTION_IDX_FILE_4 = "data/book4_faiss_section_keywords_250808.index"
SECTION_META_FILE_4 = "data/book4_meta_section_keywords_250808.pkl"
PAGE_IDX_FILE_4 = "data/book4_faiss_page_250808.index"
PAGE_META_FILE_4 = "data/book4_meta_page_250808.pkl"

with open(PAGE_META_FILE_1, "rb") as f:
    meta_pages_1 = pickle.load(f)
with open(SECTION_META_FILE_1, "rb") as f:
    meta_keywords_1 = pickle.load(f)
with open(META_FILE_1, "rb") as f:
    meta_chunks_1 = pickle.load(f)

with open(PAGE_META_FILE_2, "rb") as f:
    meta_pages_2 = pickle.load(f)
with open(SECTION_META_FILE_2, "rb") as f:
    meta_keywords_2 = pickle.load(f)
with open(META_FILE_2, "rb") as f:
    meta_chunks_2 = pickle.load(f)

with open(PAGE_META_FILE_3, "rb") as f:
    meta_pages_3 = pickle.load(f)
with open(SECTION_META_FILE_3, "rb") as f:
    meta_keywords_3 = pickle.load(f)
with open(META_FILE_3, "rb") as f:
    meta_chunks_3 = pickle.load(f)

with open(PAGE_META_FILE_4, "rb") as f:
    meta_pages_4 = pickle.load(f)
with open(SECTION_META_FILE_4, "rb") as f:
    meta_keywords_4 = pickle.load(f)
with open(META_FILE_4, "rb") as f:
    meta_chunks_4 = pickle.load(f)

PAGE_VOLUME_LIST = [("1ê¶Œ", meta_pages_1), ("2ê¶Œ", meta_pages_2), ("3ê¶Œ", meta_pages_3), ("4ê¶Œ", meta_pages_4)]
SECTION_VOLUME_LIST = [
    ("1ê¶Œ", meta_keywords_1),
    ("2ê¶Œ", meta_keywords_2),
    ("3ê¶Œ", meta_keywords_3),
    ("4ê¶Œ", meta_keywords_4),
]

#---------------------
# ë¶ˆìš©ì–´ & í† í° ìˆ˜ ì œí•œ
#---------------------
STOPWORDS = ["ì•Œë ¤", "ìˆ˜", "ìˆì–´", "ì–´ë””", "ë‚˜ì˜¤", "ëŠ”ì§€", "ì—ì„œ", "ìœ¼ë¡œ", "í•˜ê³ ", "ê°€ì´ë“œë¼ì¸", 'í™•ì¸', 'í™•ì¸í•˜ê³ ', 'ì‹¶ì–´', 'í˜ì´ì§€', 'ì–´ëŠ', 'ë¶€ë¶„', 'ë°ì´í„°']
TOKEN_LIMIT = 277000

#===================================================================================
# ê¸°ë³¸ í•¨ìˆ˜ ì„¤ì •
#===================================================================================
#---------------------
# ì„ë² ë”© íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
#---------------------
def build_or_load():
    loaded = []
    if os.path.exists(IDX_FILE_1) and os.path.exists(META_FILE_1):
        index_1 = faiss.read_index(IDX_FILE_1)
        with open(META_FILE_1, "rb") as f:
            meta_1 = pickle.load(f)
        loaded.append(("1ê¶Œ", index_1, meta_1))
    if os.path.exists(IDX_FILE_2) and os.path.exists(META_FILE_2):
        index_2 = faiss.read_index(IDX_FILE_2)
        with open(META_FILE_2, "rb") as f:
            meta_2 = pickle.load(f)
        loaded.append(("2ê¶Œ", index_2, meta_2))
    if os.path.exists(IDX_FILE_3) and os.path.exists(META_FILE_3):
        index_3 = faiss.read_index(IDX_FILE_3)
        with open(META_FILE_3, "rb") as f:
            meta_3 = pickle.load(f)
        loaded.append(("2ê¶Œ", index_3, meta_3))
    if os.path.exists(IDX_FILE_4) and os.path.exists(META_FILE_4):
        index_4 = faiss.read_index(IDX_FILE_4)
        with open(META_FILE_4, "rb") as f:
            meta_4 = pickle.load(f)
        loaded.append(("4ê¶Œ", index_4, meta_4))
    if not loaded:
        raise FileNotFoundError("ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return loaded

#---------------------
# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
#---------------------
def _embed_text(texts):
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    return [np.array(d.embedding, dtype="float32") for d in resp.data]

#===================================================================================
# ìœ„ì¹˜ / ì½”ë“œ / ë‚´ìš© ì§ˆë¬¸ êµ¬ë¶„
#===================================================================================
def classify_question(question):
    # 1. ì½”ë“œ/êµ¬í˜„ ê´€ë ¨ ì§ˆë¬¸ì¸ê°€?
    if is_code_question(question) or is_location_or_code_question_llm(question) == "YES":
        return "code"
    # 2. ìœ„ì¹˜ ê´€ë ¨ ì§ˆë¬¸ì¸ê°€?
    if is_location_question(question) or is_location_or_code_question_llm(question) == "NO":
        return "location"
    # 4. ê·¸ ì™¸ (ì„ë² ë”© ê²€ìƒ‰ ë“±)
    return "other"

#---------------------
# ì½”ë“œ ì§ˆë¬¸ì¸ì§€ í™•ì¸
#---------------------
def is_code_question(question):
    # 1ì°¨: ë‹¨ìˆœ í‚¤ì›Œë“œ ì²´í¬
    keywords = ["ìŠˆë„ì½”ë“œ", "ì½”ë“œ", "êµ¬í˜„"]
    if any(k in question for k in keywords):
        return True
    # 2ì°¨: ë‹¤ì–‘í•œ í‘œê¸°(ë„ì–´ì“°ê¸°, ì˜ì–´, ì˜¤íƒ€ ë“±) ì»¤ë²„
    if is_pseudocode(question):
        return True
    return False

#---------------------
# ìœ„ì¹˜ ì§ˆë¬¸ì¸ì§€ í™•ì¸
#---------------------
def is_location_question(question):
    keywords = ["ì–´ë””", "ì ˆ", "ìœ„ì¹˜", "ë‚˜ì™€", "í¬í•¨", "ì„¹ì…˜", "ë¶€ë¶„", "ë“¤ì–´ìˆ", "ì–¸ê¸‰", "í¬í•¨ëœ", "ìˆ˜ë¡"]
    return any(k in question for k in keywords)

#-------------------------------------
# gpt-4o-minië¡œ ìœ„ì¹˜ì¸ì§€ ì½”ë“œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨
#-------------------------------------
def is_location_or_code_question_llm(question):
    prompt = (
        'If the following question is about code, pseudocode, or implementation, answer YES.'
        'or about the location of content '
        '(such as which section, part, where, included, mentioned, etc.) answer NO.\n\n'
        f'Q: {question}\n'
    )
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=3,
    )
    answer = response.choices[0].message.content.strip().upper()
    return answer == "YES"

#===================================================================================
# ì„ë² ë”© ê´€ë ¨ í•¨ìˆ˜
#===================================================================================
#-----------------------------------------
# ìŠˆë„ ì½”ë“œ, ìˆ˜ë„ì½”ë“œ -> ìŠˆë„ì½”ë“œë¡œ target ì§€ì •
#-----------------------------------------
def is_pseudocode(query: str, threshold=0.6) -> str | bool:
    target = "ìŠˆë„ì½”ë“œ"
    # target_vec = get_embedding(target)
    target_vec = get_embedding_cached("ìŠˆë„ì½”ë“œ")  # âœ… ìºì‹œ ì‚¬ìš©

    # ì˜ì–´ í‘œí˜„ì„ í•œê¸€ì‹ìœ¼ë¡œ ì¹˜í™˜
    normalized_query = query.lower().replace("pseudo", "ìŠˆë„").replace("code", "ì½”ë“œ")

    candidates = normalized_query.split(" ")
    print(f"$$${candidates}")
    for i in range(len(candidates)):
        for j in range(i + 1, min(len(candidates), i + 2)):
            phrase = " ".join(candidates[i:j+1])

            try:
                # vec = get_embedding(phrase)
                vec = get_embedding_cached(phrase)  # âœ… ìºì‹œ ì‚¬ìš©

                sim = cosine_similarity(vec, target_vec)
                print(f"ìœ ì‚¬ë„({phrase} vs ìŠˆë„ ì½”ë“œ): {sim:.3f}")
                if sim >= threshold:
                    print(target)
                    return target
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ì˜¤ë¥˜: {e}")
                continue

    return False

#---------------------------------------------------------------
# í•œ ë²ˆ ì„ë² ë”©ì„ ê³„ì‚°í•œ í…ìŠ¤íŠ¸ëŠ” ë‹¤ì‹œ APIë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šê³  ìºì‹œì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
#---------------------------------------------------------------
cache = {}
def get_embedding_cached(text):
    if text in cache:
        return cache[text]  # ğŸ‘‰ ì €ì¥ëœ ê°’ ì¬ì‚¬ìš©
    emb = get_embedding(text)
    cache[text] = emb       # ğŸ‘‰ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
    return emb

#---------------------------------------------------
# ì–´ë–¤ ë‹¨ì–´ê°€ ìŠˆë„ì½”ë“œ ë‹¨ì–´ì™€ ìœ ì‚¬í•œì§€ íŒë³„í•˜ëŠ” ì„ë² ë”© ê¸°ë°˜ í•¨ìˆ˜
#---------------------------------------------------
def is_pseudocode_keyword(word: str, threshold=0.4) -> bool:
    # ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ íŒë³„
    target = "ìŠˆë„ì½”ë“œ"
    word = word.lower().replace("pseudo", "ìŠˆë„").replace("code", "ì½”ë“œ")
    print(f"word: {word}")
    target_vec = get_embedding(target)
    word_vec = get_embedding(word)

    sim = cosine_similarity(word_vec, target_vec)
    print(f"sim: {sim}")
    return sim >= threshold

#---------------------
# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
#---------------------
def get_embedding(text):
    return _embed_text([text])[0]

#------------------------------------
# ë‘ ì„ë² ë”© ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
#------------------------------------
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))

#===================================================================================
# RAG ê´€ë ¨ í•¨ìˆ˜
#===================================================================================
#----------------------------------------
# ì—¬ëŸ¬ ê¶Œì— ê±¸ì¹œ ë¬¸ì„œ DBì—ì„œ ê´€ë ¨ ë¬¸ë§¥ì„ ì°¾ì•„ì„œ,
# ê·¸ê±¸ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ë‹µë³€í•˜ë„ë¡ í•˜ëŠ” í•¨ìˆ˜
#----------------------------------------
def rag_chat_multi_volume(query, history, model=CHAT_MODEL_MINI):
    context_blobs = retrieve_multi_volume(query)
    # ê° ë¸”ë¡ì— ì¶œì²˜ í‘œì‹œ
    context_text = "\n\n".join(
        f"[{c['volume']}] {c['text']}" for c in context_blobs
    )

    messages = (
        [{"role": "system",
          "content": "Task: \n"
                     "You are a helpful RAG assistant. Given a user question and context, answer appropriately."

                     "Instructions: \n"
                     "1. Use only the provided context to answer the user's question."
                     "2. For the terms 'ì •ë°€ë„ (precision)' and 'ì •ë°€ì„± (preciseness)':"
                            "- Do NOT ever confuse or mix up these two terms."
                            "- Each term is a distinct metric with its own unique definition and formula."
                            "- If the question is about 'ì •ë°€ë„', only provide the definition and formula for precision."
                            "- If the question is about 'ì •ë°€ì„±', only provide the definition and formula for preciseness."
                     "3. For any formula or equation mentioned in the document:" 
                            "- Show it **exactly** as it appears in the text."    
                            "- Do not modify, rephrase, or re-typeset."
                            "- Just copy and paste the original LaTeX or expression as-is from the document."

                     "Output Format:\n"
                     "1. Write your answer as a concise, direct response."
                     "2. Keep your response brief and clear."
                     "3. If you cannot answer based on the context, reply exactly: 'ì œê°€ ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì…ë‹ˆë‹¤. ì—°êµ¬ì§„ì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.'"
          }]
        + history
        + [{"role": "user", "content": f"Context:\n{context_text}\n\nQuestion: {query}"}]
    )

    return client.chat.completions.create(
        model       = model,
        messages    = messages,
        stream      = True,
        max_tokens  = 512, # ê³ ë ¤
        temperature = 0,
    )

#----------------------------------------
# RAGì˜ ê²€ìƒ‰(R) ë¶€ë¶„
#----------------------------------------
def retrieve_multi_volume(query, top_k=4):
    q_emb = np.array(_embed_text([query])[0]).reshape(1, -1)
    results = []
    for label, index, meta in CHUNKS_VOLUME_LIST:
        D, I = index.search(q_emb, top_k)
        for idx in I[0]:
            chunk = meta[idx]
            # ì¶œì²˜ ì •ë³´ ì¶”ê°€ (ê¶Œ)
            chunk = dict(chunk)
            chunk["volume"] = label
            results.append(chunk)
    return results

#===================================================================================
# ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì´ë¼ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜
#===================================================================================
def is_insufficient_answer(answer: str) -> bool:
    return (
        "ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì…ë‹ˆë‹¤. ì—°êµ¬ì§„ì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”" in answer
    )

#===================================================================================
# ë‹µë³€ì— ì‚¬ìš©í•œ ëª¨ë¸ëª… í™•ì¸ í•¨ìˆ˜
#===================================================================================
def contains_model_tag(text, model):
    # ëª¨ë¸ëª… ë¬¸êµ¬ê°€ ì´ë¯¸ í¬í•¨ëëŠ”ì§€ ì²´í¬
    tag = f"{model}ë¡œ ë‹µë³€"
    tag2 = f"**{model}**ë¡œ ë‹µë³€"
    return (tag in text) or (tag2 in text)

#===================================================================================
# LaTeX ê´€ë ¨ í•¨ìˆ˜
#===================================================================================
#--------------------------------------------------
# í…ìŠ¤íŠ¸ì™€ LaTeX ìˆ˜ì‹ì„ ê°™ì´ ì˜ˆì˜ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë Œë”ë§ í•¨ìˆ˜
#--------------------------------------------------
def display_with_latex(text):
    # ë¸”ë¡ ìˆ˜ì‹ êµ¬ê°„ split
    blocks = re.split(r'\\\[(.*?)\\\]', text, flags=re.DOTALL)
    for i, block in enumerate(blocks):
        if i % 2 == 0:
            # ì„¤ëª…ë¬¸, ì¸ë¼ì¸ í…ìŠ¤íŠ¸
            st.write(block)
        else:
            # LaTeX ë¸”ë¡ ìˆ˜ì‹
            enhanced = enhance_korean_fraction(block.strip())
            st.latex(enhanced)

#--------------------------------------------------
# LaTeX ìˆ˜ì‹ì—ì„œ í•œê¸€ì´ í¬í•¨ëœ ë¶„ìˆ˜ í‘œí˜„ì„ ì˜ˆì˜ê²Œ ê°€ê³µ
#--------------------------------------------------
def enhance_korean_fraction(expr: str) -> str:
    # í•œê¸€ ë¬¸ìì—´ì„ \text{...}ë¡œ ê°ì‹¸ê¸°
    def wrap_korean(text: str):
        return re.sub(r"([ê°€-í£]+)", r"\\text{\1}", text)

    pattern = r"\\frac\s*{\s*(.+?)\s*}\s*{\s*(.+?)\s*}"

    def repl(match):
        numerator = wrap_korean(match.group(1)) # ë¶„ì
        denominator = wrap_korean(match.group(2)) # ë¶„ëª¨

        # displaystyle ë° ìˆ˜ì§ ì •ë ¬ ì¶”ê°€
        numerator = rf"\rule{{0pt}}{{1em}}{numerator}"
        denominator = rf"{denominator}\rule[-1em]{{0pt}}{{0pt}}"
        return rf"\displaystyle \frac{{{numerator}}}{{{denominator}}}"

    return re.sub(pattern, repl, expr)

#===================================================================================
# ì§ˆë¬¸ì´ ìœ„ì¹˜, ì½”ë“œì¸ì§€ ë“±ì„ íŒë‹¨ -> ê·¸ì— ë§ëŠ” ê²€ìƒ‰ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë„ë¡ ë¶„ê¸°
#===================================================================================
def query_by_question_subject_location_pseudo(query, question_subject):
    q_type = clean_phrase(query)

    if question_subject in ("location", "location_or_code"):
        return find_in_pages(q_type)
    elif question_subject == "code" or is_pseudocode(query) == "ìŠˆë„ì½”ë“œ":
        return find_pseudocode_sections(q_type)
    else:
        return None

#--------------------
# ì¡°ì‚¬ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜
#--------------------
def clean_phrase(phrase):
    # "ì˜", "ê°€", "ì„", "ë¥¼" ë“±ì˜ ì¡°ì‚¬ë¥¼ ëª¨ë‘ ì œê±°
    return re.sub(r'(ì˜|ê°€ |ì„|ë¥¼|ì€|ëŠ”|ì´ |ì—|ì™€|ê³¼|ë¡œ|ìœ¼ë¡œ|,)', ' ', phrase)

#------------------------------------------------------
# í•´ë‹¹ ìš©ì–´ê°€ ë“±ì¥í•˜ëŠ” í˜ì´ì§€ ëª©ë¡ì„ ë¬¸ìì—´ë¡œ ë§Œë“¤ì–´ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
#------------------------------------------------------
def find_in_pages(q_type):
    keywords_list = extract_nouns(q_type)
    n = len(keywords_list)
    answer_lines = []
    used_phrases = set()
    shown_phrases = set()  # ì´ë¯¸ í‘œì‹œí•œ í‘œê¸°(ëŒ€í‘œ í‘œê¸°, ë¶™ì—¬ì“°ê¸°/ë„ì–´ì“°ê¸° ëª¨ë‘)

    # 2ê°œ ì´ìƒ ë‹¨ì–´ë©´ ë³µí•©ì–´ ìš°ì„ !
    if n >= 2:
        phrase = " ".join(keywords_list)
        phrase_nospace = phrase.replace(" ", "")
        # ëŒ€í‘œ í‘œê¸°ëŠ” ë„ì–´ì“°ê¸° ìˆëŠ” ìª½ìœ¼ë¡œ!
        found = False
        for cand, display_phrase in [(phrase, phrase), (phrase_nospace, phrase)]:
            if display_phrase in shown_phrases:
                continue
            for label, meta_pages in PAGE_VOLUME_LIST:
                matched_pages = find_pages_with_keywords([cand], meta_pages)
                if matched_pages:
                    answer_lines.append(
                        f'**{display_phrase}**ì€(ëŠ”) **{label}** {", ".join(map(str, matched_pages))}ìª½(í˜ì´ì§€)ì— ë‚˜ì˜µë‹ˆë‹¤.\n'
                    )
                    shown_phrases.add(display_phrase)
                    found = True
        if found:
            return "\n".join(answer_lines)

    # ë³µí•©ì–´ë¡œ ëª» ì°¾ì•˜ì„ ë•Œë§Œ ë‹¨ì¼ì–´ë¡œ ê°ì ê²€ìƒ‰
    for k in keywords_list:
        if k in shown_phrases:
            continue
        for label, meta_pages in PAGE_VOLUME_LIST:
            matched_pages = find_pages_with_keywords([k], meta_pages)
            if matched_pages:
                answer_lines.append(
                    f'**{k}**ì€(ëŠ”) **{label}** {", ".join(map(str, matched_pages))}ìª½(í˜ì´ì§€)ì— ë‚˜ì˜µë‹ˆë‹¤.\n'
                )
                shown_phrases.add(k)
    return "\n".join(answer_lines) if answer_lines else "í•´ë‹¹ í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

#-----------------------------------
# ì§ˆë¬¸ì—ì„œ í•œê¸€ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜
#-----------------------------------
def extract_nouns(text):
    # ê¸°ì¡´: ëª¨ë“  2ê¸€ì ì´ìƒ í•œê¸€ ì¶”ì¶œ
    words = re.findall(r'[ê°€-í£]{2,}', text)
    # ë¶ˆìš©ì–´ ì œê±°
    return [w for w in words if w not in STOPWORDS]

#-----------------------------------------------
# ì£¼ì–´ì§„ í‚¤ì›Œë“œë“¤ì´ í¬í•¨ëœ í˜ì´ì§€ë¥¼ ì°¾ì•„ë‚´ëŠ” í•µì‹¬ ê²€ìƒ‰ í•¨ìˆ˜
#-----------------------------------------------
def find_pages_with_keywords(keywords, meta_pages):
    results = []
    if isinstance(keywords, str):
        keywords = [keywords]

    for page_meta in meta_pages:
        text = page_meta["text"]
        text_nospace = re.sub(r'\s+', '', text)  # ëª¨ë“  ê³µë°±ë¥˜ ì œê±°
        if all(
            k.replace(" ", "") in text_nospace
            for k in keywords
        ):
            results.append(page_meta["page"])
    return sorted(set(results))

#---------------------------------------
# ìŠˆë„ì½”ë“œê°€ í¬í•¨ëœ ì ˆì„ ì°¾ì•„ì£¼ëŠ” ê²€ìƒ‰ ì „ìš© í•¨ìˆ˜
#---------------------------------------
def find_pseudocode_sections(q_type):
    keywords = extract_keywords(q_type)
    print(f"keywords: {keywords}")

    concept_keywords = [k for k in keywords if not is_pseudocode_keyword(k)]
    print(f"concept_keywords: {concept_keywords}")

    phrase = " ".join(concept_keywords)
    print(f"phrase: {phrase}")

    matched_sections = []

    for label, meta_keywords in SECTION_VOLUME_LIST:
        for meta_kw in meta_keywords:
            # ë³µí•©ì–´(ë„ì–´ì“°ê¸°/ë¶™ì—¬ì“°ê¸°) ëª¨ë‘ ê²€ì‚¬
            candidates = [phrase]
            if " " in phrase:
                candidates.append(phrase.replace(" ", ""))
            for cand in candidates:
                if any(cand in item for item in meta_kw["keywords"]):
                    if "ìŠˆë„ ì½”ë“œ" in meta_kw["text"] or "ìŠˆë„ì½”ë“œ" in meta_kw["text"]:
                        matched_sections.append((label, meta_kw))

    if matched_sections:
        answers = [
            f"**{label}** {section.get('section', 'í•´ë‹¹ ì ˆ')} ì ˆì— ìŠˆë„ì½”ë“œê°€ ìˆìŠµë‹ˆë‹¤."
            for label, section in matched_sections
        ]
        return "\n\n".join(answers)
    else:
        return "í•´ë‹¹ ì ˆì—ëŠ” ìŠˆë„ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤."

#--------------------------------------------------------
# ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ gpt-4o-minië¥¼ ì´ìš©í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
#--------------------------------------------------------
def extract_keywords(question):
    prompt = (
        "From the question below, extract all the main subject, keyword, or technical term the user is asking about. "
        "Split all compound words and list every technical term separately, separated by commas. "
        "Do not group multiple terms together. "
        "Do not include any other words or explanation.\n\n"
        f"Question: {question}"
    )
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=20,
    )
    return [kw.strip() for kw in response.choices[0].message.content.strip().split(',')]

#===================================================================================
# ì‚¬ìš©ì ì‹ë³„ìš©(fp) ê´€ë ¨ í•¨ìˆ˜
#===================================================================================
TTL = int(os.getenv("CHAT_TTL_SECONDS", "1800"))  # ê¸°ë³¸ 1800ì´ˆ (30ë¶„)
CHAT_TTL_SECONDS = TTL

# ì„ íƒì‚¬í•­: ì„œë²„ì¸¡ ì†”íŠ¸ (í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ê¶Œì¥)
FP_SALT = os.getenv("FP_SALT", "please-change-this-salt")

def get_simple_fingerprint() -> Tuple[str, Dict[str, Any]]:
    """
    ë¸Œë¼ìš°ì €ì—ì„œ (1) persist_id (localStorage) (2) public IP (ipify) (3) navigator.userAgent
    ë¥¼ ê°€ì ¸ì™€ì„œ ê°„ë‹¨í•œ fingerprintë¥¼ ìƒì„±í•˜ê³  (fp, info) í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    ë°˜í™˜:
      - fp: 24ì ê¸¸ì´ì˜ hex ë¬¸ìì—´ (ë¹ˆ ë¬¸ìì—´ì´ë©´ ì‹¤íŒ¨)
      - info: ìˆ˜ì§‘ëœ ì›ë³¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (pid, ip, ua)
    """
    # JS: persist_id ìƒì„±/ì½ê¸° + ipify í˜¸ì¶œ + userAgent ìˆ˜ì§‘
    js = r"""
    (async () => {
      try {
        // 1) persist_id (localStorage)
        let pid = localStorage.getItem("persist_id");
        if (!pid) {
          // crypto.randomUUID() ì§€ì› ì•ˆ ë˜ë©´ fallback
          pid = (typeof crypto?.randomUUID === "function") ? crypto.randomUUID() : ('p_' + Math.random().toString(36).slice(2,12));
          localStorage.setItem("persist_id", pid);
        }

        // 2) public IP (ipify)
        let ip = "";
        try {
          const res = await fetch('https://api64.ipify.org?format=json');
          const j = await res.json();
          ip = j?.ip || "";
        } catch(e) {
          ip = "";
        }

        // 3) userAgent
        const ua = navigator.userAgent || "";

        return { pid, ip, ua };
      } catch (e) {
        return { pid: "", ip: "", ua: "" };
      }
    })()
    """

    try:
        info = streamlit_js_eval(js_expressions=js, key="simple_fp_collect", want_output=True) or {}
    except Exception:
        # streamlit_js_eval í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        info = {"pid": "", "ip": "", "ua": ""}

    # ë³´ì •: íƒ€ì… ì•ˆì •ì„± ìœ ì§€
    pid = str(info.get("pid", "") or "")
    ip  = str(info.get("ip", "") or "")
    ua  = str(info.get("ua", "") or "")

    info_clean: Dict[str, Any] = {"pid": pid, "ip": ip, "ua": ua}

    # fingerprint ì›ë¬¸ (ìˆœì„œ ê³ ì •) + ì„œë²„ ì†”íŠ¸ í¬í•¨
    raw = "|".join([pid, ip, ua, FP_SALT])
    fp = hashlib.sha256(raw.encode("utf-8")).hexdigest()[:24]

    # ë¹ˆê°’ ì²´í¬: pidê°€ ì „í˜€ ì—†ê³  ipë„ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŒ
    if not (pid or ip or ua):
        return "", info_clean

    return fp, info_clean

#-----------------------------------------------------------
# fpë³„ë¡œ í•˜ë£¨ ë™ì•ˆ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í† í° ìˆ˜ ì œí•œí•˜ëŠ” ì¼ì¼ í† í° ì¿¼í„° ì‹œìŠ¤í…œ
#-----------------------------------------------------------
def handle_question(prompt, ip: str):
    # ipê°€ ì•„ì§ ëª» ì¡í˜”ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ê°€ë“œ
    if not ip:
        return "IP í™•ì¸ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    key = _today_key_for_ip(ip)
    prompt_tokens = count_tokens(prompt)
    current = int(r.get(key) or 0)

    if current + prompt_tokens > TOKEN_LIMIT:
        return "ì˜¤ëŠ˜ì˜ í† í° ì‚¬ìš©ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."

    # í† í° ì¦ê°€ + ì˜¤ëŠ˜ ìì •(UTC)ê¹Œì§€ TTL ì„¤ì •
    ttl = _secs_until_utc_midnight()
    pipe = r.pipeline()
    pipe.incrby(key, prompt_tokens)
    pipe.expire(key, ttl)  # ë§¤ í˜¸ì¶œë§ˆë‹¤ ê°±ì‹ í•´ì„œ ëˆ„ë½ëœ í‚¤ë„ ì •ë¦¬
    pipe.execute()
    return "ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ"

#---------------------------------------------------
# Redisì— ì €ì¥í•  í•˜ë£¨ ë‹¨ìœ„ fpë³„ í† í° ì‚¬ìš©ëŸ‰ í‚¤ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
#---------------------------------------------------
def _today_key_for_ip(ip: str) -> str:
    today = datetime.now().strftime("%Y-%m-%d")
    safe_ip = _normalize_ip(ip) or "unknown"
    return f"tokens:{safe_ip}:{today}"

#----------------------------------------------
# fp ë¬¸ìì—´ì„ Redis í‚¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì •ì œí•˜ëŠ” í•¨ìˆ˜
#----------------------------------------------
def _normalize_ip(ip: str) -> str:
    # Redis í‚¤ì— ì•ˆì „í•˜ë„ë¡ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    return re.sub(r'[^0-9a-zA-Z\.\-_:]', '_', ip or "")

#----------------------------------------------
# íŠ¹ì • fp ì‚¬ìš©ìê°€ ì˜¤ëŠ˜ ì‚¬ìš©í•œ í† í° ìˆ˜ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
#----------------------------------------------
def get_token_usage_for_ip(ip: str):
    if not ip:
        return 0
    key = _today_key_for_ip(ip)
    return int(r.get(key) or 0)

#-------------------------------------------------
# íŠ¹ì • fpì— ëŒ€í•œ ê³¼ê±° ëŒ€í™” ê¸°ë¡ì„ Redisì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
#-------------------------------------------------
def load_chat(fp: str, max_messages: int = 100) -> list[dict]:
    """
    fingerprintë³„ ëŒ€í™” ë‚´ì—­ì„ Redisì—ì„œ ë¶ˆëŸ¬ì˜´.
    """
    key = chat_key_by_fp(fp)
    msgs = r.lrange(key, -max_messages, -1) or []
    return [json.loads(m) for m in msgs]

#------------------------------------------------------
# ì‚¬ìš©ì fpë¥¼ ê¸°ë°˜ìœ¼ë¡œ Redisì— ì €ì¥ë  ì±„íŒ… ê¸°ë¡ í‚¤ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
#------------------------------------------------------
def chat_key_by_fp(fp: str) -> str:
    """fingerprintë³„ Redis ì±„íŒ… í‚¤"""
    return f"chat:{fp}"

#---------------------------------------
# ì‚¬ìš©ìì˜ ëŒ€í™” ë©”ì‹œì§€ë¥¼ Redisì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
#---------------------------------------
def append_message(role: str, content: str, fp: str):
    """Redisì— ë©”ì‹œì§€ë¥¼ ì¶”ê°€"""
    key = chat_key_by_fp(fp)
    r.rpush(key, json.dumps({"role": role, "content": content}))
    r.expire(key, CHAT_TTL_SECONDS)

#===================================================================================
# ì§ˆë¬¸ì´ ëª‡ ê°œì˜ í† í°ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
#===================================================================================
def count_tokens(text):
    try:
        enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
    except Exception:
        enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text or ""))

#===================================================================================
# í˜„ì¬ ì‹œê°ìœ¼ë¡œë¶€í„° UTC ê¸°ì¤€ ì˜¤ëŠ˜ ìì •ê¹Œì§€ ë‚¨ì€ ì‹œê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
#===================================================================================
def _secs_until_utc_midnight() -> int:
    now = datetime.utcnow()
    tomorrow = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    return int((tomorrow - now).total_seconds())

#===================================================================================
# Streamlit UI
#===================================================================================
CHUNKS_VOLUME_LIST = build_or_load()

st.set_page_config(page_title="chatbot")
st.image("img/logo.png", width=170)
st.error("ì´ ì±—ë´‡ì€ ì°¸ê³ ìš©ìœ¼ë¡œ ì œê³µë˜ë©°, ì¤‘ìš”í•œ ë‚´ìš©ì€ ë°˜ë“œì‹œ ê³µì‹ ê°€ì´ë“œë¼ì¸ì„ í™•ì¸í•˜ì„¸ìš”.")

# fingerprint ìƒì„± (IP + UA + persist_id)
fp, info = get_simple_fingerprint()   # â† ê¸°ì¡´ get_client_ip() ëŒ€ì‹  ì‚¬ìš©

if fp:
    st.caption(f"ID: {fp[-6:]}")
    st.session_state["fingerprint"] = fp
    st.session_state["client_info"] = info
else:
    st.caption("í˜„ì¬ ID ìƒì„± ì¤‘â€¦(ë¸Œë¼ìš°ì € ì •ë³´ í™•ì¸)")
    st.stop()

# ê³¼ê±° ëŒ€í™” ì „ì²´ ì¶œë ¥ (Redisì—ì„œ ë¡œë“œ)
for h in load_chat(fp):
    with st.chat_message(h["role"]):
        display_with_latex(h["content"])

# ìƒˆ ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
if prompt := st.chat_input("ê°€ì´ë“œë¼ì¸ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”â€¦"):
    # í† í°(IP ê¸°ì¤€)
    result = handle_question(prompt, fp)
    usage = get_token_usage_for_ip(fp)
    st.markdown(f"ì˜¤ëŠ˜ ì‚¬ìš©í•œ í† í° ìˆ˜: **{usage} / {TOKEN_LIMIT}**")

    # 3. user ì§ˆë¬¸ ì¦‰ì‹œ ì¶œë ¥
    st.chat_message("user").markdown(prompt)
    append_message("user", prompt, fp)

    with st.chat_message("assistant"):
        # 1) ì¸ë””ì¼€ì´í„° í‘œì‹œ
        typing_box = st.empty()
        typing_box.markdown("""
        <style>
        .typing {font-size: 0.95rem; color: #6b7280;}
        .typing .dot {animation: blink 1.2s infinite;}
        .typing .dot:nth-child(2){animation-delay:0.2s}
        .typing .dot:nth-child(3){animation-delay:0.4s}
        @keyframes blink {0%{opacity:.2} 20%{opacity:1} 100%{opacity:.2}}
        </style>
        <div class="typing">ë‹µë³€ ìƒì„± ì¤‘ <span class="dot">â€¢</span><span class="dot">â€¢</span><span class="dot">â€¢</span></div>
        """, unsafe_allow_html=True)

        if ("í† í° ì‚¬ìš©ëŸ‰ì„ ì´ˆê³¼" in result) or ("IP í™•ì¸ ì¤‘" in result):
            typing_box.empty()  # â† ì¢…ë£Œ ì‹œ ë°˜ë“œì‹œ ì§€ì›Œì£¼ê¸°
            st.markdown(f"{result}")
            st.stop()

        question = classify_question(prompt)
        print(f"1) question: {question}")

        if question == "other":
            full_response = ""
            for chunk in rag_chat_multi_volume(prompt, load_chat(fp)):
                delta = chunk.choices[0].delta.content or ""
                full_response += delta

            if is_insufficient_answer(full_response):
                full_response = ""
                for chunk in rag_chat_multi_volume(prompt, load_chat(fp), model=CHAT_MODEL_GENERAL):
                    delta = chunk.choices[0].delta.content or ""
                    full_response += delta
                if not contains_model_tag(full_response, CHAT_MODEL_GENERAL):
                    full_response += f"\n\n**{CHAT_MODEL_GENERAL}**ë¡œ ë‹µë³€"
            else:
                if not contains_model_tag(full_response, CHAT_MODEL_MINI):
                    full_response += f"\n\n**{CHAT_MODEL_MINI}**ë¡œ ë‹µë³€"

            typing_box.empty()  # â† ë‹µë³€ ì¶œë ¥ ì§ì „ì— ì œê±°
            display_with_latex(full_response)
            append_message("assistant", full_response, fp)
            st.stop()

        # ìœ„ì¹˜/ìŠˆë„ì½”ë“œ ê²½ë¡œ
        answer = query_by_question_subject_location_pseudo(prompt, question)
        typing_box.empty()
        print(f"2) answer: {answer}")
        if answer:
            display_with_latex(answer)
            append_message("assistant", answer, fp)
            st.stop()
